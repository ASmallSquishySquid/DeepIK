"Supervised learning and systems with excess degrees of freedom":
Intro:
Inverse kinematics are one-to -many functions, because there are many possible motor positions for one end effector position
Problem: Need to pick the "best" input out of all the possible inputs for a given output
Flipping the forward kinematics solution ignores the one-to-many problem

Paper specifically discusses how to solve the problem above (choosing the best solution)

Choose optimality principle (restriction to choose the most optimum solution) based on the problem being solved, not uniqueness
Configurational constraints- Limit to the way that the object can be configured
Temporal constraints- Assume the actions are implemented in a non-conflicting way
Combine the constraints and an optimality principle into a function to get an error function to be minimized to find the optimal solution

Configurational constraints:
Arise from fixed structural relations in the manipulator and environment
Don't-care condition- no constraints
Inequality constraint- error generated by activation required to be greater than or less than a certain threshold
Ranges- sum of the error of the two inequality constraints
Linear constraints- outputs are combined linearly into another layer for target values to be applied to
Nonlinear constraints- output is transformed nonlinearly before comparison with the target

Temporal constraints:
Involves actions that are close in time
Arise from dynamic properties of the manipulator and environment
State (which varies with time) is an input to the model in addition to the given input
Smoothness constraint- minimal change to perform a sequence smoothly and quickly

Simulation:
Uses a manipulator with excess degrees of freedom in the simulations
Will only look at manipulator one, as that as the one that is relevant to the project
The excess degrees of freedom of the manipulator allow the solution to minimize temporal constraints
Was able to learn many different sequences
Learning was generalized, so learning time decreases with new sequences. Side effect: It was very sensitive to the relationship between sequences. Previously learned sequences affected the way it learned new sequences.
Resilient against errors in the model